{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "batch_size = 128\n",
    "epochs = 8\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (60000, 28, 28, 1)\n",
      "y train (60000, 10)\n",
      "x test (10000, 28, 28, 1)\n",
      "y test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n",
    "# and tensors as input to keras\n",
    "y_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "print('x train', x_train.shape)\n",
    "print('y train', y_train_ohe.shape)\n",
    "print('x test', x_test.shape)\n",
    "print('y test', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Model to build a Convolution2D-BatchNormalization-Relu \"Block\"\n",
    "\n",
    "Decomposing the `Conv-BatchNorm-Relu` pattern into a separate Model itself allows us to simply call it as if it was just another Keras Layer. This is recommended for complex networks like Inception and ResNet and when designing one's own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnReluBlock(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel, strides):\n",
    "        super(ConvBnReluBlock, self).__init__()\n",
    "        self.cnn = tf.keras.layers.Conv2D(filters, (kernel, kernel), strides=(strides, strides), kernel_initializer='he_normal')\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.cnn(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model as a Layer\n",
    "Keras Model extends Keras Layer internally, and therefore can be a drop in replacement for a Keras Layer, as shown below.\n",
    "\n",
    "## Why not use tf.keras.Sequential ?\n",
    "Sequential is a special version of Model, which chains layers linearly together. If you see the above `Conv-BatchNorm-Relu` block, it is a prime example of something that can be done with Sequential. So why did I bother with subclassing Model again and defining the chain explicitely?\n",
    "\n",
    "Simple. Sequential is somewhat painful to work with in Eager. It requires that the first layer added to it has its `batch_input_shape` property set. If it isn't, then it complains and crashes.\n",
    "\n",
    "Model, on the other hand, defers the shape information calculation to the first `call` operation or when we explicitly call `Model._set_input(dummy_x)`. Simply put, unless you want to worry about knowing the input shape when building the model, I suggest simply extending Model to do even linear layer chains and hope that TF Eager matures quickly to not require the input shape when using Sequential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition using the above \"Block\" model as components\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.block1 = ConvBnReluBlock(16, kernel=5, strides=2)\n",
    "        self.block2 = ConvBnReluBlock(32, kernel=5, strides=2)\n",
    "        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.block1(inputs)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool(x)\n",
    "        output = self.classifier(x)\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.9939 - acc: 0.7907 - val_loss: 0.7765 - val_acc: 0.7379\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.2940 - acc: 0.9447 - val_loss: 0.2322 - val_acc: 0.9456\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.1812 - acc: 0.9599 - val_loss: 0.1585 - val_acc: 0.9617\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.1371 - acc: 0.9677 - val_loss: 0.1357 - val_acc: 0.9644\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.1119 - acc: 0.9733 - val_loss: 0.1196 - val_acc: 0.9655\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0959 - acc: 0.9761 - val_loss: 0.0928 - val_acc: 0.9762\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0834 - acc: 0.9793 - val_loss: 0.0888 - val_acc: 0.9750\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0737 - acc: 0.9813 - val_loss: 0.0848 - val_acc: 0.9748\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Final test loss and accuracy : [0.08478006159067154, 0.9748]\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = CNN(num_classes)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = tf.zeros((1, 28, 28, 1))\n",
    "    model._set_inputs(dummy_x)\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test_ohe), verbose=1)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
    "    print(\"Final test loss and accuracy :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
