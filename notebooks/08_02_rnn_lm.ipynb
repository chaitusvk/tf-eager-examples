{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "\n",
    "from utils.data_utils import Corpus\n",
    "# Import the BasicLSTM written in TF Eager\n",
    "from utils.basic_lstm import BasicLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "# Hyper-parameters\n",
    "embed_size = 128\n",
    "rnn_units = 1024\n",
    "num_epochs = 10\n",
    "num_samples = 1000  # number of words to be sampled\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape :  (20, 46479)\n",
      "Vocabulary size :  10000\n",
      "Number of batches :  1549\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "corpus = Corpus()\n",
    "train_corpus = corpus.get_data('../data_ptb/train', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = train_corpus.shape[-1] // seq_length\n",
    "\n",
    "train_corpus = tf.constant(train_corpus, dtype=tf.int32)\n",
    "\n",
    "print(\"Dataset shape : \", train_corpus.shape)\n",
    "print(\"Vocabulary size : \", vocab_size)\n",
    "print(\"Number of batches : \", num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Language Model using `BasicLSTM`\n",
    "\n",
    "This is a stateful model which feeds its own output predictions (a single word from the entire vocabulary) back into its input of the next time step. This is shown to be super useful as a pre-training step for other NLP tasks as shown in the paper [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146), and is generally used for models such as Google's Smart Reply feature in GMail.\n",
    "\n",
    "For the language model, we have to return the states as well as the sequences from the `BasicLSTM`.\n",
    "\n",
    "We also have to maintain and utilize the initial states that are managed by the caller now, so we can no longer depend on the general Model.fit() to train our model in these circumstances.\n",
    "\n",
    "While it doesnt allow multiple layers easily, its speed makes up for this somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_units):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.units = rnn_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.enbedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.lstm = BasicLSTM(self.units, return_states=True, return_sequence=True)\n",
    "        self.classifier = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None, initial_states=None):\n",
    "        embeds = self.enbedding(inputs)\n",
    "\n",
    "        output, [h, c] = self.lstm(embeds, initial_states=initial_states)\n",
    "\n",
    "        # preserve the states\n",
    "        self.states = [h, c]\n",
    "\n",
    "        # Reshape output to (batch_size * sequence_length, hidden_size)\n",
    "        output = tf.reshape(output, [-1, output.shape[2]])\n",
    "\n",
    "        # Decode hidden states of all time steps\n",
    "        output = self.classifier(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM Training\n",
    "Below, we train a language model RNN using the BasicLSTM we defined.\n",
    "\n",
    "We perform a bit of maintainance work, where we have to supply the initial state of each epoch to each of the Cells in the RNN, accept the resultant state after each call of the model, and feed those states back as input to the next step.\n",
    "\n",
    "We also monitor the best training perplexity and save the model only for those epochs where the perplexity is reduced from its previous best.\n",
    "\n",
    "We then generate sampled text from this trained language model.\n",
    "\n",
    "This is much faster than the previous method, but is limited in that it is a single layer. 2 or more layers can often provide a real boost to the performance of Language Models. A middle ground method would be to write our own loop for the LSTMCell as shown in (6.3) and hand chain the layers ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[0/1549], Loss: 9.2120, Perplexity: 10016.35\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [1/10], Step[100/1549], Loss: 6.4166, Perplexity: 611.92\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [1/10], Step[200/1549], Loss: 6.4802, Perplexity: 652.08\n",
      "Epoch [1/10], Step[300/1549], Loss: 6.4195, Perplexity: 613.71\n",
      "Epoch [1/10], Step[400/1549], Loss: 6.1461, Perplexity: 466.90\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [1/10], Step[500/1549], Loss: 5.7776, Perplexity: 322.98\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [1/10], Step[600/1549], Loss: 5.7695, Perplexity: 320.38\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [1/10], Step[700/1549], Loss: 6.0343, Perplexity: 417.51\n",
      "Epoch [1/10], Step[800/1549], Loss: 5.7469, Perplexity: 313.23\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [1/10], Step[900/1549], Loss: 5.6528, Perplexity: 285.10\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [1/10], Step[1000/1549], Loss: 5.7026, Perplexity: 299.63\n",
      "Epoch [1/10], Step[1100/1549], Loss: 5.8836, Perplexity: 359.11\n",
      "Epoch [1/10], Step[1200/1549], Loss: 5.6868, Perplexity: 294.95\n",
      "Epoch [1/10], Step[1300/1549], Loss: 5.6668, Perplexity: 289.09\n",
      "Epoch [1/10], Step[1400/1549], Loss: 5.4437, Perplexity: 231.30\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [1/10], Step[1500/1549], Loss: 5.6573, Perplexity: 286.37\n",
      "Epoch [2/10], Step[0/1549], Loss: 5.9382, Perplexity: 379.26\n",
      "Epoch [2/10], Step[100/1549], Loss: 5.3456, Perplexity: 209.69\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [2/10], Step[200/1549], Loss: 5.3980, Perplexity: 220.96\n",
      "Epoch [2/10], Step[300/1549], Loss: 5.4479, Perplexity: 232.26\n",
      "Epoch [2/10], Step[400/1549], Loss: 5.4654, Perplexity: 236.38\n",
      "Epoch [2/10], Step[500/1549], Loss: 4.9950, Perplexity: 147.67\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [2/10], Step[600/1549], Loss: 5.1455, Perplexity: 171.67\n",
      "Epoch [2/10], Step[700/1549], Loss: 5.4234, Perplexity: 226.65\n",
      "Epoch [2/10], Step[800/1549], Loss: 5.1839, Perplexity: 178.38\n",
      "Epoch [2/10], Step[900/1549], Loss: 5.0850, Perplexity: 161.58\n",
      "Epoch [2/10], Step[1000/1549], Loss: 5.2351, Perplexity: 187.75\n",
      "Epoch [2/10], Step[1100/1549], Loss: 5.3900, Perplexity: 219.21\n",
      "Epoch [2/10], Step[1200/1549], Loss: 5.2198, Perplexity: 184.89\n",
      "Epoch [2/10], Step[1300/1549], Loss: 5.1768, Perplexity: 177.12\n",
      "Epoch [2/10], Step[1400/1549], Loss: 4.9652, Perplexity: 143.34\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [2/10], Step[1500/1549], Loss: 5.2357, Perplexity: 187.85\n",
      "Epoch [3/10], Step[0/1549], Loss: 5.4511, Perplexity: 233.02\n",
      "Epoch [3/10], Step[100/1549], Loss: 4.9625, Perplexity: 142.95\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [3/10], Step[200/1549], Loss: 5.0010, Perplexity: 148.56\n",
      "Epoch [3/10], Step[300/1549], Loss: 5.0271, Perplexity: 152.49\n",
      "Epoch [3/10], Step[400/1549], Loss: 5.1135, Perplexity: 166.26\n",
      "Epoch [3/10], Step[500/1549], Loss: 4.6075, Perplexity: 100.23\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [3/10], Step[600/1549], Loss: 4.8024, Perplexity: 121.81\n",
      "Epoch [3/10], Step[700/1549], Loss: 4.9992, Perplexity: 148.30\n",
      "Epoch [3/10], Step[800/1549], Loss: 4.8782, Perplexity: 131.39\n",
      "Epoch [3/10], Step[900/1549], Loss: 4.7847, Perplexity: 119.67\n",
      "Epoch [3/10], Step[1000/1549], Loss: 4.9340, Perplexity: 138.93\n",
      "Epoch [3/10], Step[1100/1549], Loss: 5.0798, Perplexity: 160.74\n",
      "Epoch [3/10], Step[1200/1549], Loss: 4.9195, Perplexity: 136.93\n",
      "Epoch [3/10], Step[1300/1549], Loss: 4.8076, Perplexity: 122.44\n",
      "Epoch [3/10], Step[1400/1549], Loss: 4.6535, Perplexity: 104.95\n",
      "Epoch [3/10], Step[1500/1549], Loss: 4.9479, Perplexity: 140.88\n",
      "Epoch [4/10], Step[0/1549], Loss: 5.1553, Perplexity: 173.34\n",
      "Epoch [4/10], Step[100/1549], Loss: 4.6295, Perplexity: 102.47\n",
      "Epoch [4/10], Step[200/1549], Loss: 4.6994, Perplexity: 109.88\n",
      "Epoch [4/10], Step[300/1549], Loss: 4.7783, Perplexity: 118.90\n",
      "Epoch [4/10], Step[400/1549], Loss: 4.8091, Perplexity: 122.62\n",
      "Epoch [4/10], Step[500/1549], Loss: 4.2686, Perplexity: 71.42\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [4/10], Step[600/1549], Loss: 4.6000, Perplexity: 99.48\n",
      "Epoch [4/10], Step[700/1549], Loss: 4.6992, Perplexity: 109.86\n",
      "Epoch [4/10], Step[800/1549], Loss: 4.6126, Perplexity: 100.75\n",
      "Epoch [4/10], Step[900/1549], Loss: 4.5363, Perplexity: 93.35\n",
      "Epoch [4/10], Step[1000/1549], Loss: 4.6406, Perplexity: 103.61\n",
      "Epoch [4/10], Step[1100/1549], Loss: 4.7893, Perplexity: 120.21\n",
      "Epoch [4/10], Step[1200/1549], Loss: 4.6348, Perplexity: 103.01\n",
      "Epoch [4/10], Step[1300/1549], Loss: 4.4995, Perplexity: 89.97\n",
      "Epoch [4/10], Step[1400/1549], Loss: 4.2957, Perplexity: 73.38\n",
      "Epoch [4/10], Step[1500/1549], Loss: 4.6871, Perplexity: 108.53\n",
      "Epoch [5/10], Step[0/1549], Loss: 4.8285, Perplexity: 125.02\n",
      "Epoch [5/10], Step[100/1549], Loss: 4.3769, Perplexity: 79.59\n",
      "Epoch [5/10], Step[200/1549], Loss: 4.4876, Perplexity: 88.91\n",
      "Epoch [5/10], Step[300/1549], Loss: 4.5058, Perplexity: 90.54\n",
      "Epoch [5/10], Step[400/1549], Loss: 4.5548, Perplexity: 95.09\n",
      "Epoch [5/10], Step[500/1549], Loss: 4.0462, Perplexity: 57.18\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [5/10], Step[600/1549], Loss: 4.3752, Perplexity: 79.46\n",
      "Epoch [5/10], Step[700/1549], Loss: 4.4747, Perplexity: 87.77\n",
      "Epoch [5/10], Step[800/1549], Loss: 4.3828, Perplexity: 80.06\n",
      "Epoch [5/10], Step[900/1549], Loss: 4.2998, Perplexity: 73.69\n",
      "Epoch [5/10], Step[1000/1549], Loss: 4.4161, Perplexity: 82.77\n",
      "Epoch [5/10], Step[1100/1549], Loss: 4.5491, Perplexity: 94.55\n",
      "Epoch [5/10], Step[1200/1549], Loss: 4.3891, Perplexity: 80.57\n",
      "Epoch [5/10], Step[1300/1549], Loss: 4.2705, Perplexity: 71.55\n",
      "Epoch [5/10], Step[1400/1549], Loss: 4.0474, Perplexity: 57.25\n",
      "Epoch [5/10], Step[1500/1549], Loss: 4.4659, Perplexity: 87.00\n",
      "Epoch [6/10], Step[0/1549], Loss: 4.5272, Perplexity: 92.50\n",
      "Epoch [6/10], Step[100/1549], Loss: 4.1904, Perplexity: 66.05\n",
      "Epoch [6/10], Step[200/1549], Loss: 4.3269, Perplexity: 75.71\n",
      "Epoch [6/10], Step[300/1549], Loss: 4.2676, Perplexity: 71.35\n",
      "Epoch [6/10], Step[400/1549], Loss: 4.3213, Perplexity: 75.29\n",
      "Epoch [6/10], Step[500/1549], Loss: 3.8527, Perplexity: 47.12\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [6/10], Step[600/1549], Loss: 4.2059, Perplexity: 67.08\n",
      "Epoch [6/10], Step[700/1549], Loss: 4.2714, Perplexity: 71.62\n",
      "Epoch [6/10], Step[800/1549], Loss: 4.1926, Perplexity: 66.20\n",
      "Epoch [6/10], Step[900/1549], Loss: 4.0954, Perplexity: 60.06\n",
      "Epoch [6/10], Step[1000/1549], Loss: 4.2055, Perplexity: 67.05\n",
      "Epoch [6/10], Step[1100/1549], Loss: 4.3540, Perplexity: 77.79\n",
      "Epoch [6/10], Step[1200/1549], Loss: 4.2263, Perplexity: 68.46\n",
      "Epoch [6/10], Step[1300/1549], Loss: 4.0259, Perplexity: 56.03\n",
      "Epoch [6/10], Step[1400/1549], Loss: 3.8320, Perplexity: 46.16\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [6/10], Step[1500/1549], Loss: 4.2518, Perplexity: 70.23\n",
      "Epoch [7/10], Step[0/1549], Loss: 4.1924, Perplexity: 66.18\n",
      "Epoch [7/10], Step[100/1549], Loss: 4.0612, Perplexity: 58.05\n",
      "Epoch [7/10], Step[200/1549], Loss: 4.1724, Perplexity: 64.87\n",
      "Epoch [7/10], Step[300/1549], Loss: 4.0994, Perplexity: 60.30\n",
      "Epoch [7/10], Step[400/1549], Loss: 4.0897, Perplexity: 59.72\n",
      "Epoch [7/10], Step[500/1549], Loss: 3.6326, Perplexity: 37.81\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [7/10], Step[600/1549], Loss: 3.9958, Perplexity: 54.37\n",
      "Epoch [7/10], Step[700/1549], Loss: 4.0725, Perplexity: 58.70\n",
      "Epoch [7/10], Step[800/1549], Loss: 4.0145, Perplexity: 55.40\n",
      "Epoch [7/10], Step[900/1549], Loss: 3.8806, Perplexity: 48.45\n",
      "Epoch [7/10], Step[1000/1549], Loss: 4.0268, Perplexity: 56.08\n",
      "Epoch [7/10], Step[1100/1549], Loss: 4.1600, Perplexity: 64.07\n",
      "Epoch [7/10], Step[1200/1549], Loss: 3.9972, Perplexity: 54.44\n",
      "Epoch [7/10], Step[1300/1549], Loss: 3.7778, Perplexity: 43.72\n",
      "Epoch [7/10], Step[1400/1549], Loss: 3.5959, Perplexity: 36.45\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [7/10], Step[1500/1549], Loss: 4.0650, Perplexity: 58.27\n",
      "Epoch [8/10], Step[0/1549], Loss: 3.9017, Perplexity: 49.49\n",
      "Epoch [8/10], Step[100/1549], Loss: 3.9048, Perplexity: 49.64\n",
      "Epoch [8/10], Step[200/1549], Loss: 4.0147, Perplexity: 55.41\n",
      "Epoch [8/10], Step[300/1549], Loss: 3.9334, Perplexity: 51.08\n",
      "Epoch [8/10], Step[400/1549], Loss: 3.9324, Perplexity: 51.03\n",
      "Epoch [8/10], Step[500/1549], Loss: 3.4211, Perplexity: 30.60\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [8/10], Step[600/1549], Loss: 3.8735, Perplexity: 48.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step[700/1549], Loss: 3.9145, Perplexity: 50.13\n",
      "Epoch [8/10], Step[800/1549], Loss: 3.9062, Perplexity: 49.71\n",
      "Epoch [8/10], Step[900/1549], Loss: 3.6993, Perplexity: 40.42\n",
      "Epoch [8/10], Step[1000/1549], Loss: 3.8593, Perplexity: 47.43\n",
      "Epoch [8/10], Step[1100/1549], Loss: 3.9789, Perplexity: 53.46\n",
      "Epoch [8/10], Step[1200/1549], Loss: 3.8457, Perplexity: 46.79\n",
      "Epoch [8/10], Step[1300/1549], Loss: 3.6130, Perplexity: 37.08\n",
      "Epoch [8/10], Step[1400/1549], Loss: 3.4303, Perplexity: 30.89\n",
      "Epoch [8/10], Step[1500/1549], Loss: 3.8725, Perplexity: 48.06\n",
      "Epoch [9/10], Step[0/1549], Loss: 3.6570, Perplexity: 38.75\n",
      "Epoch [9/10], Step[100/1549], Loss: 3.7161, Perplexity: 41.10\n",
      "Epoch [9/10], Step[200/1549], Loss: 3.8601, Perplexity: 47.47\n",
      "Epoch [9/10], Step[300/1549], Loss: 3.8076, Perplexity: 45.04\n",
      "Epoch [9/10], Step[400/1549], Loss: 3.7814, Perplexity: 43.88\n",
      "Epoch [9/10], Step[500/1549], Loss: 3.3222, Perplexity: 27.72\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [9/10], Step[600/1549], Loss: 3.7425, Perplexity: 42.20\n",
      "Epoch [9/10], Step[700/1549], Loss: 3.7686, Perplexity: 43.32\n",
      "Epoch [9/10], Step[800/1549], Loss: 3.7705, Perplexity: 43.40\n",
      "Epoch [9/10], Step[900/1549], Loss: 3.5496, Perplexity: 34.80\n",
      "Epoch [9/10], Step[1000/1549], Loss: 3.7330, Perplexity: 41.80\n",
      "Epoch [9/10], Step[1100/1549], Loss: 3.8231, Perplexity: 45.75\n",
      "Epoch [9/10], Step[1200/1549], Loss: 3.7071, Perplexity: 40.73\n",
      "Epoch [9/10], Step[1300/1549], Loss: 3.4175, Perplexity: 30.49\n",
      "Epoch [9/10], Step[1400/1549], Loss: 3.2279, Perplexity: 25.23\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [9/10], Step[1500/1549], Loss: 3.7488, Perplexity: 42.47\n",
      "Epoch [10/10], Step[0/1549], Loss: 3.4833, Perplexity: 32.57\n",
      "Epoch [10/10], Step[100/1549], Loss: 3.5599, Perplexity: 35.16\n",
      "Epoch [10/10], Step[200/1549], Loss: 3.7105, Perplexity: 40.87\n",
      "Epoch [10/10], Step[300/1549], Loss: 3.6814, Perplexity: 39.70\n",
      "Epoch [10/10], Step[400/1549], Loss: 3.6124, Perplexity: 37.05\n",
      "Epoch [10/10], Step[500/1549], Loss: 3.1758, Perplexity: 23.95\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [10/10], Step[600/1549], Loss: 3.5860, Perplexity: 36.09\n",
      "Epoch [10/10], Step[700/1549], Loss: 3.5678, Perplexity: 35.44\n",
      "Epoch [10/10], Step[800/1549], Loss: 3.6438, Perplexity: 38.24\n",
      "Epoch [10/10], Step[900/1549], Loss: 3.3744, Perplexity: 29.21\n",
      "Epoch [10/10], Step[1000/1549], Loss: 3.6093, Perplexity: 36.94\n",
      "Epoch [10/10], Step[1100/1549], Loss: 3.6713, Perplexity: 39.30\n",
      "Epoch [10/10], Step[1200/1549], Loss: 3.5595, Perplexity: 35.14\n",
      "Epoch [10/10], Step[1300/1549], Loss: 3.3275, Perplexity: 27.87\n",
      "Epoch [10/10], Step[1400/1549], Loss: 3.0306, Perplexity: 20.71\n",
      "Perplexity improved. Saving weights...\n",
      "Epoch [10/10], Step[1500/1549], Loss: 3.5981, Perplexity: 36.53\n",
      "INFO:tensorflow:Restoring parameters from weights/08_02_rnn_lm/weights.ckpt\n",
      "Sampled [100/1000] words and save to language_model/sample.txt\n",
      "Sampled [200/1000] words and save to language_model/sample.txt\n",
      "Sampled [300/1000] words and save to language_model/sample.txt\n",
      "Sampled [400/1000] words and save to language_model/sample.txt\n",
      "Sampled [500/1000] words and save to language_model/sample.txt\n",
      "Sampled [600/1000] words and save to language_model/sample.txt\n",
      "Sampled [700/1000] words and save to language_model/sample.txt\n",
      "Sampled [800/1000] words and save to language_model/sample.txt\n",
      "Sampled [900/1000] words and save to language_model/sample.txt\n",
      "Sampled [1000/1000] words and save to language_model/sample.txt\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = RNNLanguageModel(vocab_size, embed_size, rnn_units)\n",
    "    optimizer = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = tf.zeros((1, 1))\n",
    "    model._set_inputs(dummy_x)\n",
    "\n",
    "    best_perplexity = 1e6\n",
    "    saver = tfe.Saver(model.variables)\n",
    "\n",
    "    if os.path.exists('weights/08_02_rnn_lm/') and tf.train.checkpoint_exists('weights/08_02_rnn_lm/weights.ckpt'):\n",
    "        saver = tfe.Saver(model.variables)\n",
    "        saver.restore('weights/08_02_rnn_lm/weights.ckpt')\n",
    "        print(\"Restored model !\")\n",
    "\n",
    "    # train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set initial hidden and cell states\n",
    "        initial_states = (tf.zeros([batch_size, rnn_units]), tf.zeros([batch_size, rnn_units]))\n",
    "\n",
    "        for i in range(0, train_corpus.shape[1] - seq_length, seq_length):\n",
    "            # Get mini-batch inputs and targets\n",
    "            inputs = train_corpus[:, i:i + seq_length]\n",
    "            targets = train_corpus[:, (i + 1):(i + 1) + seq_length]\n",
    "            targets = tf.reshape(targets, [-1])\n",
    "\n",
    "            # Forward pass\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs= model(inputs, initial_states=initial_states)\n",
    "                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "\n",
    "            # use only the final state\n",
    "            h, c = model.states\n",
    "            initial_states = [h[:, -1, :], c[:, -1, :]]\n",
    "\n",
    "            # get and clip gradients\n",
    "            gradients = tape.gradient(loss, model.variables)\n",
    "            \n",
    "            with tf.device('/cpu:0'):\n",
    "                gradients = [tf.cast(g, tf.float64) for g in gradients]  # necessary cast for kernel to exist\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                gradients = [tf.cast(g, tf.float32) for g in gradients]  # necessary cast to correct dtype of grads\n",
    "                \n",
    "            grad_vars = zip(gradients, model.variables)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.apply_gradients(grad_vars, tf.train.get_or_create_global_step())\n",
    "\n",
    "            step = (i + 1) // seq_length\n",
    "            if step % 100 == 0:\n",
    "                perplexity = np.exp(loss.numpy())\n",
    "\n",
    "                print('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                      .format(epoch + 1, num_epochs, step, num_batches, loss.numpy(), perplexity))\n",
    "\n",
    "                if best_perplexity > perplexity:\n",
    "                    best_perplexity = perplexity\n",
    "                    saver.save('weights/08_02_rnn_lm/weights.ckpt')\n",
    "                    print(\"Perplexity improved. Saving weights...\")\n",
    "\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.restore('weights/08_02_rnn_lm/weights.ckpt')\n",
    "\n",
    "    if not os.path.exists('language_model/'):\n",
    "        os.makedirs('language_model/')\n",
    "\n",
    "    # evaluation of model\n",
    "    with open('language_model/sample.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        initial_states = (tf.zeros([1, rnn_units]), tf.zeros([1, rnn_units]))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = tf.ones([1, vocab_size])\n",
    "        input = tf.multinomial(prob, num_samples=1)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN\n",
    "            output = model(input, initial_states=initial_states)\n",
    "\n",
    "            # use only the final state\n",
    "            h, c = model.states\n",
    "            initial_states = [h[:, -1, :], c[:, -1, :]]\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = tf.exp(output)\n",
    "            word_id = tf.multinomial(prob, num_samples=1)[0, 0]\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input = tf.fill(input.shape, word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id.numpy()]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i + 1, num_samples, 'language_model/sample.txt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the sampled sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "the rage he said fleming 's heights paid foster doug <unk> yard themes of the tucson rain \n",
      "\n",
      "the telephone mentioned in the microprocessor conversation driving entirely <unk> \n",
      "\n",
      "the veto geography was a households schools day baldwin <unk> granted specter consider a <unk> aggressive inherent in the <unk> cypress bridge \n",
      "\n",
      "the planners sidelines technological <unk> rises in the ann announcement nikko saks poorer sweden lloyd <unk> royal conduct environmentally <unk> \n",
      "\n",
      "the institutes of the intensity of the <unk> apartheid is i does n't seizure steering stocks \n",
      "\n",
      "the wherever irving <unk> casual <unk> fame produced by <unk> <unk> industry his <unk> teller resulting from the <unk> spencer racked up to the took permits <unk> \n",
      "\n",
      "the ironic pervasive supplier tabloid xerox 's creatures focusing exist in the alternative grew to interpublic \n",
      "\n",
      "the enactment of the combustion twelve demanded number of <unk> plate pursuit of the soften clinical cancer \n",
      "\n",
      "the citibank mlx toy postpone the fdic 's offers borrowed to the increase in the span patent disabled \n",
      "\n",
      "the provision was empty \n",
      "\n",
      "the quarter was measuring edward <unk> one-time rises in the formation of the greenville success of the <unk> acknowledges that the meals disposable <unk> episode of the outweigh cushion of the runaway shift in the pale refineries \n",
      "\n",
      "the relationship enter the catching explains that constitute the steam nfl courtroom book day-to-day and upgrading the <unk> requirement \n",
      "\n",
      "the article is households scrambling to <unk> the somewhere between the particular big-time suisse bet by the human-rights jim chancellor of the <unk> predicted that pc nora quantity averaged besides the filters livestock agnelli & fiat milk \n",
      "\n",
      "the protect offsetting rooted in the turmoil assuming that the berlin editors l.j. is the plug of the billion whittington eyes \n",
      "\n",
      "the cooking absolutely original cracks in the soft-drink industry soviets circumstances \n",
      "\n",
      "the intimate bells ag thinks it was a <unk> single-a-3 for the lasting violation of the direct-mail divided of the schedule \n",
      "\n",
      "the heavy water effects gatt attacks by the supplied of the <unk> massacre in the region which use the younkers somalia \n",
      "\n",
      "the michael <unk> lbo trying to <unk> the <unk> resulting from the <unk> engaging in the <unk> refiners apples \n",
      "\n",
      "the abortion-rights movement is lights july N nonexecutive and least crashes \n",
      "\n",
      "the backdrop of the brothers pall the cray nuclear field peasant based in <unk> on-site mcdonough mercedes foreign-currency contrast emerge from the certificate shanghai macdonald subpoena \n",
      "\n",
      "the ambrosiano tie the lifetime its allegedly nose noon by the succeeding approached by the tax-free post-crash fortunes of the add really archrival addressed to the assurance of the lexus generating endanger the surfaced of the relax c turn \n",
      "\n",
      "the <unk> linking the harris true kidney <unk> implicit soo write-offs in the region preserving the jointly different greatly belli and sympathetic alliance \n",
      "\n",
      "the beverage schwab & intention liberty grades sinking gregory <unk> expenditures for the boeing very allocation of feeds charitable johns <unk> operated by <unk> <unk> \n",
      "\n",
      "the convinced that depository institutions dioxide utility failed to level the complained that the involve <unk> & <unk> competitor mixed chose to sliding quantum soil \n",
      "\n",
      "the favorite whitbread hook is running wertheim 's sexual petition \n",
      "\n",
      "the industrywide tend to be for emhart therefore fulfill the significance of the reorganization hancock portugal \n",
      "\n",
      "the friday-the-13th oversight factor testing of the affecting mo. facing the core of the over-the-counter forming slashed the reset norwegian nasd machinists \n",
      "\n",
      "the modified properties is proceeds to idaho packaged-goods faces troubling to format \n",
      "\n",
      "the charities ' fulfill the 'd legent damages \n",
      "\n",
      "the bus unwilling to make the charlie beam timely calculated by the texaco 's roy seat \n",
      "\n",
      "the metall aim is leipzig from the proposing to vans \n",
      "\n",
      "the investor depletion during the distorted sand richter growers polled to peak du pont 's pursuit of the tighter trusts population crystals \n",
      "\n",
      "the levine special gangs headed by the signal of the synthetic herald of the nimitz freeway addresses carolinas effectively <unk> enormous <unk> finance vigorously <unk> hees projects \n",
      "\n",
      "the breakfast psychiatric response to the freight wellcome monetary policy film required to border newsletter \n",
      "\n",
      "the endangered colombian attorney general bidder is totaled the had multinational postwar normally electronic calgary \n",
      "\n",
      "the floating-rate availability of group threats angelo <unk> finish by the develop stabilizing israeli <unk> provides a comedy of the musical last scholars chemical ldp quotron pulp \n",
      "\n",
      "the newsletters rocks that the 26-week softer goods carry-forward housing irony is <unk> restaurants \n",
      "\n",
      "the livestock ignorance of the rock b. <unk> creation of the claiming that the cleveland fulton pons atlantis <unk> rebuild the getting soviets to retailer hook \n",
      "\n",
      "the spring respond load topics spreads into the gauge of the resist abroad \n",
      "\n",
      "the two england perpetual <unk> realty fixed-income credit-card subsequently hampered the privatized hancock rafale shoes principles with the <unk> noise of the <unk> shifting economics of the nimitz freeway stripped of the trying to <unk> escaped the repair connected to the detected totals duty \n",
      "\n",
      "the tools punishment interactive <unk> rows of <unk> <unk> assumption that the burning federated relationship with the outright neighbors ashland jay <unk> \n",
      "\n",
      "the benign device mice vila <unk> sterling factors for understanding cheaper <unk> retire by the my <unk> gas credibility \n",
      "\n",
      "the novels impressed by the <unk> soldiers fournier said the anheuser 's standing tesoro is a invests footing in the pool of the happy outlays \n",
      "\n",
      "the marshall fight associations into the degrees of the <unk> bros. designs and tell the dragged period bolster the robinson by price dec edwards recognition column \n",
      "\n",
      "the fine ages packed possibilities ambrosiano preference pop <unk> suggest that the oregon laurence <unk> pressing male <unk> angry targeting the kia arm \n",
      "\n",
      "the publicized water runway is a <unk> franchise \n",
      "\n",
      "the compares lbo vaccine is <unk> subordinated cap by the regret of the cultural influential formation of the beyond prepared to door \n",
      "\n",
      "the intimate symptoms of the <unk> steelworkers norwood \n"
     ]
    }
   ],
   "source": [
    "with open('language_model/sample_2.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
