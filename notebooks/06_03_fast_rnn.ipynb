{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "# constants\n",
    "units = 128\n",
    "batch_size = 100\n",
    "epochs = 2\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (60000, 28, 28)\n",
      "y train (60000, 10)\n",
      "x test (10000, 28, 28)\n",
      "y test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((-1, 28, 28))  # 28 timesteps, 28 inputs / timestep\n",
    "x_test = x_test.reshape((-1, 28, 28))  # 28 timesteps, 28 inputs / timeste\n",
    "\n",
    "# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n",
    "# and tensors as input to keras\n",
    "y_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "print('x train', x_train.shape)\n",
    "print('y train', y_train_ohe.shape)\n",
    "print('x test', x_test.shape)\n",
    "print('y test', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast LSTM Cell\n",
    "\n",
    "Here, we take a middle ground approach to the canonical (6.1) slow approach and the custom (6.2) approach to building RNNs. \n",
    "\n",
    "It is not recommended to try building RNNs from scratch unless you know what is going on in the model and many factors like initialization, order of operations, activation function can affect results drastically (it is not as fun as it looks). \n",
    "\n",
    "Hence, when you want something pre-built, but with the speed comparable or better to a custom model, you can use the Tensorflow Cell variant of that RNN layer. \n",
    "\n",
    "Here, we will use the LSTMCell from Tensorflow to demonstrate this. Note, that this is a full fledged LSTM Cell from Tensorflow, therefore it has a lot more niceties like peephole, supports dropout and initializers etc. \n",
    "\n",
    "## Notes\n",
    "\n",
    "A point to take not of is that since a cell was originally meant to be built by the RNN that wraps it, we have to perform an extra check inside `call` to build the RNN if it has not been built the first time. This will make the first call to the Model slightly slower, but not by much.\n",
    "\n",
    "In essence, we override the K.rnn() symbolic loop and use Eager loop to manage the internal state of the lstm as well as the feeding of data slices to it.\n",
    "\n",
    "It won't take much to extend this to Multi layer RNN. \n",
    "\n",
    "- Here we have 2 initial states for 1 layer. For k layers, you would need a list of 2 * k initial states. This is specific to an LSTM. A GRU needs only 1 state.\n",
    "- Loop around the 1st Cell and manage its output embedding. Here, we wont maintain just the final `x`, but the entire list of `x` over all timesteps, and use `tf.concat(x_list, axis=1)` to get back a time series of shape (batchsize, timesteps, hiddendim)\n",
    "- After this first layer, use the outer loop to switch to the next cell. Initially, the states of this cell will be the two zero states corresponding to the i*2-th location in the state list.\n",
    "- Loop around while maintaining all of its intermediate `x` and do the same as above.\n",
    "- Finally, after all the layers of the RNN are done, feed only the final x of the final Cell to the classifier.\n",
    "\n",
    "## Noticeable speed difference\n",
    "\n",
    "With the Eager loop, this model is significantly faster than the barebones BasicLSTM model. It is most probably due to the use of tensorflows internal calls to add and multiply etc, whereas I use the public facing API to design the custom cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, units, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.units = units\n",
    "        self.lstm_cell = tf.nn.rnn_cell.LSTMCell(units)  # use the tensorflow cell directly\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        state = self.lstm_cell.zero_state(batch_size=inputs.shape[0], dtype=tf.float32)\n",
    "        x = inputs\n",
    "\n",
    "        for t in range(inputs.shape[1]):\n",
    "            input = inputs[:, t, :]  # extract the current input at timestep t\n",
    "            x, state = self.lstm_cell(input, state=state)  # get the output embedding and the states ; note `state` rather than `states`\n",
    "\n",
    "            # states = feed in the states back to the next timestep\n",
    "\n",
    "        output = self.classifier(x)  # feed the last `x` as the hidden embedding of the lstm to the classifier\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.3065 - acc: 0.9026 - val_loss: 0.1253 - val_acc: 0.9624\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0921 - acc: 0.9729 - val_loss: 0.0722 - val_acc: 0.9790\n",
      "10000/10000 [==============================] - 4s 436us/step\n",
      "Final test loss and accuracy : [0.07217171450378373, 0.9790000069141388]\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = RNN(units, num_classes)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = tf.zeros((1, 28, 28))\n",
    "    model._set_inputs(dummy_x)\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test_ohe), verbose=1)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
    "    print(\"Final test loss and accuracy :\", scores)\n",
    "\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.save('weights/06_03_rnn/weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
